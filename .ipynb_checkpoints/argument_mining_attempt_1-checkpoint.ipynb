{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a80ce98-ad0c-462e-a371-e64c81c9dfe9",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f818da48-3414-4baf-b719-20b541f43e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "### importing lazypredict library\n",
    "import lazypredict\n",
    "### importing LazyClassifier for classification problem\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "### importing LazyClassifier for classification problem because here we are solving Classification use case.\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "### importing breast Cancer Dataset from sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "### spliting dataset into training and testing part\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d37b48-24ea-4e64-a276-e9e19a14db67",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aa8b59c-eec4-40ac-a393-cbf1de90e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(data):\n",
    "    print(\"[preProcess] start\")\n",
    "    print(data['sentence'][2800])\n",
    "    data['sentence'] = [entry.lower() for entry in data['sentence']]\n",
    "    data['sentence'] = [word_tokenize(entry) for entry in data['sentence']]\n",
    "    print(data['sentence'][2800])\n",
    "    \n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    for index,entry in enumerate(data['sentence']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        finalWords = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        wordLemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = wordLemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                finalWords.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        data.loc[index,'sentence'] = str(finalWords)\n",
    "    print(\"[preProcess] end\")\n",
    "    return data\n",
    "\n",
    "def vectorizeAndSplit(allData):\n",
    "    \n",
    "    print(\"[vectorizeAndSplit] start\")\n",
    "    allData = preProcess(allData)\n",
    "    display(allData)\n",
    "    allData.reset_index(drop=True,inplace=True)\n",
    "    trainingData = allData[allData['set'] == 'train']\n",
    "    validationData = allData[allData['set'] == 'val']\n",
    "    testingData = allData[allData['set'] == 'test']\n",
    "\n",
    "    trainingData.reset_index(drop=True,inplace=True)\n",
    "    validationData.reset_index(drop=True,inplace=True)\n",
    "    testingData.reset_index(drop=True,inplace=True)\n",
    "    assert( len(trainingData) + len(testingData) + len(validationData) == len(allData))\n",
    "    \n",
    "    Encoder = LabelEncoder()\n",
    "    trainY = Encoder.fit_transform(trainingData['annotation'])\n",
    "    testY = Encoder.fit_transform(testingData['annotation'])\n",
    "    \n",
    "    tfidVect = TfidfVectorizer(max_features=5000)\n",
    "    tfidVect.fit(trainingData['sentence'])\n",
    "    \n",
    "    trainXVectorized = tfidVect.transform(trainingData['sentence'])\n",
    "    testXVectorized = tfidVect.transform(testingData['sentence'])\n",
    "    \n",
    "    print(\"[vectorizeAndSplit] end\")\n",
    "    return trainXVectorized,testXVectorized,trainY,testY\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9566d9-5b3d-4633-8a69-4fc10933918b",
   "metadata": {},
   "source": [
    "### Load all data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e9d86e-57f1-4b5d-9462-13efe6ccd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allData = pd.DataFrame()\n",
    "dataDirectory = 'data'\n",
    "for filename in os.listdir(dataDirectory):\n",
    "    f = os.path.join(dataDirectory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        df = pd.read_csv(f,sep = '\\t',quoting=csv.QUOTE_NONE)\n",
    "        allData = pd.concat([allData,df])\n",
    "allData.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799c8fd-149c-45e3-8bab-e5377b89d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXVectorized,testXVectorized,trainY,testY = vectorizeAndSplit(allData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b44e7b-ef3e-498c-b055-aa64cd262e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes F1 Score ->  0.6265413975337639\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(trainXVectorized,trainY)\n",
    "# predict the labels on validation dataset\n",
    "predictionsNB = Naive.predict(testXVectorized)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes F1 Score -> \",f1_score(predictionsNB, testY,pos_label='positive',average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6f3bff-221f-4172-ae62-e4347efff9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM F1 Score ->  62.86944607555295\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(trainXVectorized,trainY)\n",
    "# predict the labels on validation dataset\n",
    "predictionsSVM = SVM.predict(testXVectorized)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM F1 Score -> \",f1_score(predictionsSVM, testY,average='micro')*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6291cdf3-d0f6-4f41-b4cc-b7b563ed68e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost F1 Score ->  63.28048541789\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier() \n",
    "model.fit(trainXVectorized, trainY)\n",
    "predXGBoost = model.predict(testXVectorized)\n",
    "print(\"XGBoost F1 Score -> \",f1_score(predXGBoost, testY,average='micro')*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d537cbe2-4fbf-4f97-9341-269a99059a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, ..., 2, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainXVectorized.shape, trainY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba6847-5f39-4738-a1b4-8c7a82035b5e",
   "metadata": {},
   "source": [
    "# With Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585b8816-3689-48c6-86d0-aa8e139ae746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 67.0/67.0 [00:00<00:00, 67.0kB/s]\n",
      "Downloading (…)/main/vocab-src.json: 100%|██████████| 849k/849k [00:06<00:00, 132kB/s]\n",
      "Downloading (…)/main/vocab-tgt.json: 100%|██████████| 849k/849k [00:03<00:00, 243kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 315k/315k [00:01<00:00, 181kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT:  The quick brown fox jumps over the lazy dog .\n",
      "AUGMENTED TEXT:  ['The speedy brown fox jumps over the lazy dog.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aug = naw.BackTranslationAug()\n",
    "text = 'The quick brown fox jumps over the lazy dog .'\n",
    "\n",
    "print(\"ORIGINAL TEXT: \", text)\n",
    "print(\"AUGMENTED TEXT: \",aug.augment(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d3daeb-ac39-4fee-b838-c1f60102b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backTranslationAugmentation(data):\n",
    "    \n",
    "    duplicatedDf = pd.concat([data]*2, ignore_index=True)\n",
    "    for i in range( int ( len(duplicatedDf)/2 ) , len(duplicatedDf)):\n",
    "        if(i==0):\n",
    "            print('Duplicate : ',duplicatedDf.loc[i,'sentence'])\n",
    "            print('Original : ',duplicatedDf.loc[i - int ( len(duplicatedDf)/2 ) ,'sentence'])\n",
    "        duplicatedDf.loc[i,'sentence'] = aug.augment( duplicatedDf.loc[i - int ( len(duplicatedDf)/2 ) ,'sentence'] )\n",
    "    duplicatedDf.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    return duplicatedDf\n",
    "\n",
    "def vectorizeAugmentAndSplit(allData):\n",
    "    \n",
    "    print(\"[vectorizeAndSplit] start\")\n",
    "    allData = preProcess(allData)\n",
    "    display(allData)\n",
    "    allData.reset_index(drop=True,inplace=True)\n",
    "    trainingData = allData[allData['set'] == 'train']\n",
    "    validationData = allData[allData['set'] == 'val']\n",
    "    testingData = allData[allData['set'] == 'test']\n",
    "\n",
    "    trainingData.reset_index(drop=True,inplace=True)\n",
    "    trainingData = backTranslationAugmentation(trainingData)\n",
    "    validationData.reset_index(drop=True,inplace=True)\n",
    "    testingData.reset_index(drop=True,inplace=True)\n",
    "    assert( len(trainingData)/2 + len(testingData) + len(validationData) == len(allData))\n",
    "    \n",
    "    Encoder = LabelEncoder()\n",
    "    trainY = Encoder.fit_transform(trainingData['annotation'])\n",
    "    testY = Encoder.fit_transform(testingData['annotation'])\n",
    "    \n",
    "    tfidVect = TfidfVectorizer(max_features=5000)\n",
    "    tfidVect.fit(trainingData['sentence'])\n",
    "    \n",
    "    trainXVectorized = tfidVect.transform(trainingData['sentence'])\n",
    "    testXVectorized = tfidVect.transform(testingData['sentence'])\n",
    "    \n",
    "    print(\"[vectorizeAndSplit] end\")\n",
    "    return trainXVectorized,testXVectorized,trainY,testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d295ec7-4a6b-49ea-97fc-2bd72c8414a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "allData = pd.DataFrame()\n",
    "dataDirectory = 'data'\n",
    "for filename in os.listdir(dataDirectory):\n",
    "    f = os.path.join(dataDirectory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        df = pd.read_csv(f,sep = '\\t',quoting=csv.QUOTE_NONE)\n",
    "        allData = pd.concat([allData,df])\n",
    "allData.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5282c78-53cb-4e16-8102-85d79f0f1b18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vectorizeAndSplit] start\n",
      "[preProcess] start\n",
      "But harming or killing another against his will , not by free contract , is clearly wrong ; if that is n't wrong , what is ?\n",
      "['but', 'harming', 'or', 'killing', 'another', 'against', 'his', 'will', ',', 'not', 'by', 'free', 'contract', ',', 'is', 'clearly', 'wrong', ';', 'if', 'that', 'is', \"n't\", 'wrong', ',', 'what', 'is', '?']\n",
      "[preProcess] end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic</th>\n",
       "      <th>retrievedUrl</th>\n",
       "      <th>archivedUrl</th>\n",
       "      <th>sentenceHash</th>\n",
       "      <th>sentence</th>\n",
       "      <th>annotation</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>abortion</td>\n",
       "      <td>http://2012election.procon.org/view.additional-resource.php?resourceID=004933</td>\n",
       "      <td>http://web.archive.org/web/20150415052859/http://2012election.procon.org/view.additional-resource.php?resourceID=004933</td>\n",
       "      <td>a1d2d5656a5029eb558812b8259b6567</td>\n",
       "      <td>['mean', 'steer', 'monetary', 'policy', 'keep', 'price', 'stable', 'b', 'keep', 'unemployment', 'low', 'economy', 'grow']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>abortion</td>\n",
       "      <td>http://www.listland.com/top-10-arguments-in-support-of-abortion/</td>\n",
       "      <td>http://web.archive.org/web/20160829133344/http://www.listland.com/top-10-arguments-in-support-of-abortion/</td>\n",
       "      <td>a4374eb8cae2c1d52499d0489c7bfb1d</td>\n",
       "      <td>['get']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>abortion</td>\n",
       "      <td>http://www.americamagazine.org/issue/feminist-case-against-abortion</td>\n",
       "      <td>http://web.archive.org/web/20160422223822/http://americamagazine.org:80/issue/feminist-case-against-abortion</td>\n",
       "      <td>825b1a5e0e7915950a2a4a657230d530</td>\n",
       "      <td>['nathanson', 'later', 'become']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>abortion</td>\n",
       "      <td>http://www.strangenotions.com/answering-three-common-arguments-for-abortion/</td>\n",
       "      <td>http://web.archive.org/web/20160916225634/http://www.strangenotions.com/answering-three-common-arguments-for-abortion/</td>\n",
       "      <td>644379f8e228f50f0871270164878c9b</td>\n",
       "      <td>['case', 'may', 'never', 'evil', 'directly', 'attack', 'kill', 'child', 'via', 'abortion', 'good', 'save', 'life', 'mother', 'may', 'result']</td>\n",
       "      <td>Argument_against</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>abortion</td>\n",
       "      <td>http://www.healthguidance.org/entry/13561/1/Pros-and-Cons-of-Abortion.html</td>\n",
       "      <td>http://web.archive.org/web/20160425042210/http://www.healthguidance.org:80/entry/13561/1/Pros-and-Cons-of-Abortion.html</td>\n",
       "      <td>51eefb36e8947e42403e336536cb00f0</td>\n",
       "      <td>['would', 'like', 'give', 'everyone', 'something', 'contemplate']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25487</th>\n",
       "      <td>3003</td>\n",
       "      <td>school uniforms</td>\n",
       "      <td>http://collegefootball.procon.org/</td>\n",
       "      <td>http://web.archive.org/web/20160621012142/http://collegefootball.procon.org:80/</td>\n",
       "      <td>34b6c3b9d4cb4bcad22bd525af887122</td>\n",
       "      <td>['playoff', 'would', 'remove', 'easy', 'schedule', 'make', 'championship', 'solely', 'performance']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25488</th>\n",
       "      <td>3004</td>\n",
       "      <td>school uniforms</td>\n",
       "      <td>http://debatepedia.idebate.org/en/index.php/Debate:_School_uniform</td>\n",
       "      <td>http://web.archive.org/web/20160709035437/http://debatepedia.idebate.org:80/en/index.php/Debate:_School_uniform</td>\n",
       "      <td>2489c5583b3489c2a931e40aba0176e7</td>\n",
       "      <td>['example', 'sikh', 'boy', 'orthodox', 'jew', 'islamic', 'girl', 'express', 'religious', 'belief', 'way', 'dress', 'uniform', 'stop']</td>\n",
       "      <td>Argument_against</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25489</th>\n",
       "      <td>3005</td>\n",
       "      <td>school uniforms</td>\n",
       "      <td>https://www.bostonglobe.com/metro/regionals/south/2014/02/20/uniforms-for-public-school-students-the-debate-continues-they-become-more-popular/btIYoFLoizOkFpXrdEUScK/story.html</td>\n",
       "      <td>http://web.archive.org/web/20160302063501/http://www.bostonglobe.com:80/metro/regionals/south/2014/02/20/uniforms-for-public-school-students-the-debate-continues-they-become-more-popular/btIYoFLoizOkFpXrdEUScK/story.html?</td>\n",
       "      <td>d32cd0154bd3d610fe5c8b255a30d39c</td>\n",
       "      <td>['lot', 'parental', 'input', 'say', 'bransfield']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25490</th>\n",
       "      <td>3006</td>\n",
       "      <td>school uniforms</td>\n",
       "      <td>http://www.educationbug.org/a/public-school-uniform-debate.html</td>\n",
       "      <td>http://web.archive.org/web/20160724094732/http://www.educationbug.org:80/a/public-school-uniform-debate.html</td>\n",
       "      <td>76498681c603d4c75690a2f25322106e</td>\n",
       "      <td>['school', 'uniform', 'ugly', 'unflattering', 'wear', 'something', 'unattractive', 'unflattering', 'good', 'student']</td>\n",
       "      <td>Argument_against</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25491</th>\n",
       "      <td>3007</td>\n",
       "      <td>school uniforms</td>\n",
       "      <td>https://journalistsresource.org/studies/society/education/school-uniforms-research-student-achievement-behavior</td>\n",
       "      <td>http://web.archive.org/web/20170126112226/https://journalistsresource.org/studies/society/education/school-uniforms-research-student-achievement-behavior</td>\n",
       "      <td>3a183cc644d9c5f790ea95c27c4f5e09</td>\n",
       "      <td>['public', 'school', 'nationwide', 'focus', 'improve', 'standardized', 'test', 'score', 'campus', 'safety', 'grow', 'number', 'begin', 'require', 'student', 'wear', 'uniforms', 'typically', 'polo', 'shirt', 'particular', 'color', 'pair', 'navy', 'khaki', 'pant', 'skirt', 'short']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25492 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index            topic  \\\n",
       "0          0         abortion   \n",
       "1          1         abortion   \n",
       "2          2         abortion   \n",
       "3          3         abortion   \n",
       "4          4         abortion   \n",
       "...      ...              ...   \n",
       "25487   3003  school uniforms   \n",
       "25488   3004  school uniforms   \n",
       "25489   3005  school uniforms   \n",
       "25490   3006  school uniforms   \n",
       "25491   3007  school uniforms   \n",
       "\n",
       "                                                                                                                                                                           retrievedUrl  \\\n",
       "0                                                                                                         http://2012election.procon.org/view.additional-resource.php?resourceID=004933   \n",
       "1                                                                                                                      http://www.listland.com/top-10-arguments-in-support-of-abortion/   \n",
       "2                                                                                                                   http://www.americamagazine.org/issue/feminist-case-against-abortion   \n",
       "3                                                                                                          http://www.strangenotions.com/answering-three-common-arguments-for-abortion/   \n",
       "4                                                                                                            http://www.healthguidance.org/entry/13561/1/Pros-and-Cons-of-Abortion.html   \n",
       "...                                                                                                                                                                                 ...   \n",
       "25487                                                                                                                                                http://collegefootball.procon.org/   \n",
       "25488                                                                                                                http://debatepedia.idebate.org/en/index.php/Debate:_School_uniform   \n",
       "25489  https://www.bostonglobe.com/metro/regionals/south/2014/02/20/uniforms-for-public-school-students-the-debate-continues-they-become-more-popular/btIYoFLoizOkFpXrdEUScK/story.html   \n",
       "25490                                                                                                                   http://www.educationbug.org/a/public-school-uniform-debate.html   \n",
       "25491                                                                   https://journalistsresource.org/studies/society/education/school-uniforms-research-student-achievement-behavior   \n",
       "\n",
       "                                                                                                                                                                                                                         archivedUrl  \\\n",
       "0                                                                                                            http://web.archive.org/web/20150415052859/http://2012election.procon.org/view.additional-resource.php?resourceID=004933   \n",
       "1                                                                                                                         http://web.archive.org/web/20160829133344/http://www.listland.com/top-10-arguments-in-support-of-abortion/   \n",
       "2                                                                                                                       http://web.archive.org/web/20160422223822/http://americamagazine.org:80/issue/feminist-case-against-abortion   \n",
       "3                                                                                                             http://web.archive.org/web/20160916225634/http://www.strangenotions.com/answering-three-common-arguments-for-abortion/   \n",
       "4                                                                                                            http://web.archive.org/web/20160425042210/http://www.healthguidance.org:80/entry/13561/1/Pros-and-Cons-of-Abortion.html   \n",
       "...                                                                                                                                                                                                                              ...   \n",
       "25487                                                                                                                                                http://web.archive.org/web/20160621012142/http://collegefootball.procon.org:80/   \n",
       "25488                                                                                                                http://web.archive.org/web/20160709035437/http://debatepedia.idebate.org:80/en/index.php/Debate:_School_uniform   \n",
       "25489  http://web.archive.org/web/20160302063501/http://www.bostonglobe.com:80/metro/regionals/south/2014/02/20/uniforms-for-public-school-students-the-debate-continues-they-become-more-popular/btIYoFLoizOkFpXrdEUScK/story.html?   \n",
       "25490                                                                                                                   http://web.archive.org/web/20160724094732/http://www.educationbug.org:80/a/public-school-uniform-debate.html   \n",
       "25491                                                                      http://web.archive.org/web/20170126112226/https://journalistsresource.org/studies/society/education/school-uniforms-research-student-achievement-behavior   \n",
       "\n",
       "                           sentenceHash  \\\n",
       "0      a1d2d5656a5029eb558812b8259b6567   \n",
       "1      a4374eb8cae2c1d52499d0489c7bfb1d   \n",
       "2      825b1a5e0e7915950a2a4a657230d530   \n",
       "3      644379f8e228f50f0871270164878c9b   \n",
       "4      51eefb36e8947e42403e336536cb00f0   \n",
       "...                                 ...   \n",
       "25487  34b6c3b9d4cb4bcad22bd525af887122   \n",
       "25488  2489c5583b3489c2a931e40aba0176e7   \n",
       "25489  d32cd0154bd3d610fe5c8b255a30d39c   \n",
       "25490  76498681c603d4c75690a2f25322106e   \n",
       "25491  3a183cc644d9c5f790ea95c27c4f5e09   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                       sentence  \\\n",
       "0                                                                                                                                                                     ['mean', 'steer', 'monetary', 'policy', 'keep', 'price', 'stable', 'b', 'keep', 'unemployment', 'low', 'economy', 'grow']   \n",
       "1                                                                                                                                                                                                                                                                                       ['get']   \n",
       "2                                                                                                                                                                                                                                                              ['nathanson', 'later', 'become']   \n",
       "3                                                                                                                                                 ['case', 'may', 'never', 'evil', 'directly', 'attack', 'kill', 'child', 'via', 'abortion', 'good', 'save', 'life', 'mother', 'may', 'result']   \n",
       "4                                                                                                                                                                                                                             ['would', 'like', 'give', 'everyone', 'something', 'contemplate']   \n",
       "...                                                                                                                                                                                                                                                                                         ...   \n",
       "25487                                                                                                                                                                                       ['playoff', 'would', 'remove', 'easy', 'schedule', 'make', 'championship', 'solely', 'performance']   \n",
       "25488                                                                                                                                                     ['example', 'sikh', 'boy', 'orthodox', 'jew', 'islamic', 'girl', 'express', 'religious', 'belief', 'way', 'dress', 'uniform', 'stop']   \n",
       "25489                                                                                                                                                                                                                                         ['lot', 'parental', 'input', 'say', 'bransfield']   \n",
       "25490                                                                                                                                                                     ['school', 'uniform', 'ugly', 'unflattering', 'wear', 'something', 'unattractive', 'unflattering', 'good', 'student']   \n",
       "25491  ['public', 'school', 'nationwide', 'focus', 'improve', 'standardized', 'test', 'score', 'campus', 'safety', 'grow', 'number', 'begin', 'require', 'student', 'wear', 'uniforms', 'typically', 'polo', 'shirt', 'particular', 'color', 'pair', 'navy', 'khaki', 'pant', 'skirt', 'short']   \n",
       "\n",
       "             annotation    set  \n",
       "0            NoArgument    val  \n",
       "1            NoArgument  train  \n",
       "2            NoArgument    val  \n",
       "3      Argument_against  train  \n",
       "4            NoArgument   test  \n",
       "...                 ...    ...  \n",
       "25487        NoArgument  train  \n",
       "25488  Argument_against  train  \n",
       "25489        NoArgument  train  \n",
       "25490  Argument_against  train  \n",
       "25491        NoArgument  train  \n",
       "\n",
       "[25492 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainXVectorized,testXVectorized,trainY,testY \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizeAugmentAndSplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallData\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 24\u001b[0m, in \u001b[0;36mvectorizeAugmentAndSplit\u001b[1;34m(allData)\u001b[0m\n\u001b[0;32m     21\u001b[0m testingData \u001b[38;5;241m=\u001b[39m allData[allData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m trainingData\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 24\u001b[0m trainingData \u001b[38;5;241m=\u001b[39m \u001b[43mbackTranslationAugmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m validationData\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m testingData\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m, in \u001b[0;36mbackTranslationAugmentation\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuplicate : \u001b[39m\u001b[38;5;124m'\u001b[39m,duplicatedDf\u001b[38;5;241m.\u001b[39mloc[i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal : \u001b[39m\u001b[38;5;124m'\u001b[39m,duplicatedDf\u001b[38;5;241m.\u001b[39mloc[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mint\u001b[39m ( \u001b[38;5;28mlen\u001b[39m(duplicatedDf)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m ) ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m     duplicatedDf\u001b[38;5;241m.\u001b[39mloc[i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43maug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mduplicatedDf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mduplicatedDf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m duplicatedDf\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m duplicatedDf\n",
      "File \u001b[1;32m~\\Desktop\\programming\\usask\\LoganChallenge\\NLP\\nlp\\Lib\\site-packages\\nlpaug\\base_augmenter.py:98\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[1;34m(self, data, n, num_thread)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstSummAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBackTranslationAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextualWordEmbsAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextualWordEmbsForSentenceAug\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(aug_num):\n\u001b[1;32m---> 98\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43maction_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    100\u001b[0m             augmented_results\u001b[38;5;241m.\u001b[39mextend(result)\n",
      "File \u001b[1;32m~\\Desktop\\programming\\usask\\LoganChallenge\\NLP\\nlp\\Lib\\site-packages\\nlpaug\\augmenter\\word\\back_translation.py:70\u001b[0m, in \u001b[0;36mBackTranslationAug.substitute\u001b[1;34m(self, data, n)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m---> 70\u001b[0m augmented_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m augmented_text\n",
      "File \u001b[1;32m~\\Desktop\\programming\\usask\\LoganChallenge\\NLP\\nlp\\Lib\\site-packages\\nlpaug\\model\\lang_models\\machine_translation_transformers.py:39\u001b[0m, in \u001b[0;36mMtTransformers.predict\u001b[1;34m(self, texts, target_words, n)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts, target_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 39\u001b[0m     src_translated_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_one_step_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     tgt_translated_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslate_one_step_batched(src_translated_texts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_tokenizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_model)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tgt_translated_texts\n",
      "File \u001b[1;32m~\\Desktop\\programming\\usask\\LoganChallenge\\NLP\\nlp\\Lib\\site-packages\\nlpaug\\model\\lang_models\\machine_translation_transformers.py:62\u001b[0m, in \u001b[0;36mMtTransformers.translate_one_step_batched\u001b[1;34m(self, data, tokenizer, model)\u001b[0m\n\u001b[0;32m     59\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(t\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[0;32m     60\u001b[0m         input_ids, attention_mask \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 62\u001b[0m         translated_ids_batch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m         all_translated_ids\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     68\u001b[0m             translated_ids_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     69\u001b[0m         )\n\u001b[0;32m     71\u001b[0m all_translated_texts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\Desktop\\programming\\usask\\LoganChallenge\\NLP\\nlp\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\programming\\usask\\LoganChallenge\\NLP\\nlp\\Lib\\site-packages\\transformers\\generation\\utils.py:1490\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1484\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1485\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   1486\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1487\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1488\u001b[0m     )\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   1504\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32m~\\Desktop\\programming\\usask\\LoganChallenge\\NLP\\nlp\\Lib\\site-packages\\transformers\\generation\\utils.py:2822\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2818\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   2819\u001b[0m     outputs, model_kwargs, is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder\n\u001b[0;32m   2820\u001b[0m )\n\u001b[0;32m   2821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2822\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_key_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate \u001b[38;5;129;01mand\u001b[39;00m output_scores:\n\u001b[0;32m   2825\u001b[0m     beam_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[38;5;241m+\u001b[39m (beam_idx[i],) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(beam_indices))))\n",
      "File \u001b[1;32m~\\Desktop\\programming\\usask\\LoganChallenge\\NLP\\nlp\\Lib\\site-packages\\transformers\\models\\fsmt\\modeling_fsmt.py:1301\u001b[0m, in \u001b[0;36mFSMTForConditionalGeneration._reorder_cache\u001b[1;34m(past_key_values, beam_idx)\u001b[0m\n\u001b[0;32m   1298\u001b[0m reordered_past \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past \u001b[38;5;129;01min\u001b[39;00m past_key_values:\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;66;03m# get the correct batch idx from decoder layer's batch dim for cross and self-attn\u001b[39;00m\n\u001b[1;32m-> 1301\u001b[0m     layer_past_new \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_key\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_reorder_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattn_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m   1304\u001b[0m     reordered_past\u001b[38;5;241m.\u001b[39mappend(layer_past_new)\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reordered_past\n",
      "File \u001b[1;32m~\\Desktop\\programming\\usask\\LoganChallenge\\NLP\\nlp\\Lib\\site-packages\\transformers\\models\\fsmt\\modeling_fsmt.py:1302\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1298\u001b[0m reordered_past \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past \u001b[38;5;129;01min\u001b[39;00m past_key_values:\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;66;03m# get the correct batch idx from decoder layer's batch dim for cross and self-attn\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m     layer_past_new \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m-> 1302\u001b[0m         attn_key: \u001b[43m_reorder_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m attn_key, attn_cache \u001b[38;5;129;01min\u001b[39;00m layer_past\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1303\u001b[0m     }\n\u001b[0;32m   1304\u001b[0m     reordered_past\u001b[38;5;241m.\u001b[39mappend(layer_past_new)\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reordered_past\n",
      "File \u001b[1;32m~\\Desktop\\programming\\usask\\LoganChallenge\\NLP\\nlp\\Lib\\site-packages\\transformers\\models\\fsmt\\modeling_fsmt.py:852\u001b[0m, in \u001b[0;36m_reorder_buffer\u001b[1;34m(attn_cache, new_order)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, input_buffer_k \u001b[38;5;129;01min\u001b[39;00m attn_cache\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_buffer_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 852\u001b[0m         attn_cache[k] \u001b[38;5;241m=\u001b[39m \u001b[43minput_buffer_k\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_order\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_cache\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainXVectorized,testXVectorized,trainY,testY = vectorizeAugmentAndSplit(allData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8deb66b-e820-49c3-80e4-d6562c9fd192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f793736-dff7-431a-ae93-e970b2589908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
