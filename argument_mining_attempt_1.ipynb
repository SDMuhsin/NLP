{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a80ce98-ad0c-462e-a371-e64c81c9dfe9",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f818da48-3414-4baf-b719-20b541f43e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "### importing lazypredict library\n",
    "import lazypredict\n",
    "### importing LazyClassifier for classification problem\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "### importing LazyClassifier for classification problem because here we are solving Classification use case.\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "### importing breast Cancer Dataset from sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "### spliting dataset into training and testing part\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d37b48-24ea-4e64-a276-e9e19a14db67",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aa8b59c-eec4-40ac-a393-cbf1de90e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(data):\n",
    "    print(\"[preProcess] start\")\n",
    "    print(data['sentence'][2800])\n",
    "    data['sentence'] = [entry.lower() for entry in data['sentence']]\n",
    "    data['sentence'] = [word_tokenize(entry) for entry in data['sentence']]\n",
    "    print(data['sentence'][2800])\n",
    "    \n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    for index,entry in enumerate(data['sentence']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        finalWords = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        wordLemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = wordLemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                finalWords.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        data.loc[index,'sentence'] = str(finalWords)\n",
    "    print(\"[preProcess] end\")\n",
    "    return data\n",
    "\n",
    "def vectorizeAndSplit(allData):\n",
    "    \n",
    "    print(\"[vectorizeAndSplit] start\")\n",
    "    allData = preProcess(allData)\n",
    "    display(allData)\n",
    "    allData.reset_index(drop=True,inplace=True)\n",
    "    trainingData = allData[allData['set'] == 'train']\n",
    "    validationData = allData[allData['set'] == 'val']\n",
    "    testingData = allData[allData['set'] == 'test']\n",
    "\n",
    "    trainingData.reset_index(drop=True,inplace=True)\n",
    "    validationData.reset_index(drop=True,inplace=True)\n",
    "    testingData.reset_index(drop=True,inplace=True)\n",
    "    assert( len(trainingData) + len(testingData) + len(validationData) == len(allData))\n",
    "    \n",
    "    Encoder = LabelEncoder()\n",
    "    trainY = Encoder.fit_transform(trainingData['annotation'])\n",
    "    testY = Encoder.fit_transform(testingData['annotation'])\n",
    "    \n",
    "    tfidVect = TfidfVectorizer(max_features=5000)\n",
    "    tfidVect.fit(trainingData['sentence'])\n",
    "    \n",
    "    trainXVectorized = tfidVect.transform(trainingData['sentence'])\n",
    "    testXVectorized = tfidVect.transform(testingData['sentence'])\n",
    "    \n",
    "    print(\"[vectorizeAndSplit] end\")\n",
    "    return trainXVectorized,testXVectorized,trainY,testY\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9566d9-5b3d-4633-8a69-4fc10933918b",
   "metadata": {},
   "source": [
    "### Load all data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e9d86e-57f1-4b5d-9462-13efe6ccd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allData = pd.DataFrame()\n",
    "dataDirectory = 'data'\n",
    "for filename in os.listdir(dataDirectory):\n",
    "    f = os.path.join(dataDirectory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        df = pd.read_csv(f,sep = '\\t',quoting=csv.QUOTE_NONE)\n",
    "        allData = pd.concat([allData,df])\n",
    "allData.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799c8fd-149c-45e3-8bab-e5377b89d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXVectorized,testXVectorized,trainY,testY = vectorizeAndSplit(allData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b44e7b-ef3e-498c-b055-aa64cd262e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes F1 Score ->  0.6265413975337639\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(trainXVectorized,trainY)\n",
    "# predict the labels on validation dataset\n",
    "predictionsNB = Naive.predict(testXVectorized)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes F1 Score -> \",f1_score(predictionsNB, testY,pos_label='positive',average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6f3bff-221f-4172-ae62-e4347efff9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM F1 Score ->  62.86944607555295\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(trainXVectorized,trainY)\n",
    "# predict the labels on validation dataset\n",
    "predictionsSVM = SVM.predict(testXVectorized)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM F1 Score -> \",f1_score(predictionsSVM, testY,average='micro')*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6291cdf3-d0f6-4f41-b4cc-b7b563ed68e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost F1 Score ->  63.28048541789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = XGBClassifier() \n",
    "model.fit(trainXVectorized, trainY)\n",
    "predXGBoost = model.predict(testXVectorized)\n",
    "print(\"XGBoost F1 Score -> \",f1_score(predXGBoost, testY,average='micro')*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d537cbe2-4fbf-4f97-9341-269a99059a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, ..., 2, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainXVectorized.shape, trainY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba6847-5f39-4738-a1b4-8c7a82035b5e",
   "metadata": {},
   "source": [
    "# With Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585b8816-3689-48c6-86d0-aa8e139ae746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 67.0/67.0 [00:00<00:00, 67.0kB/s]\n",
      "Downloading (…)/main/vocab-src.json: 100%|██████████| 849k/849k [00:06<00:00, 132kB/s]\n",
      "Downloading (…)/main/vocab-tgt.json: 100%|██████████| 849k/849k [00:03<00:00, 243kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 315k/315k [00:01<00:00, 181kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT:  The quick brown fox jumps over the lazy dog .\n",
      "AUGMENTED TEXT:  ['The speedy brown fox jumps over the lazy dog.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aug = naw.BackTranslationAug()\n",
    "text = 'The quick brown fox jumps over the lazy dog .'\n",
    "\n",
    "print(\"ORIGINAL TEXT: \", text)\n",
    "print(\"AUGMENTED TEXT: \",aug.augment(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4663adfb-8e6c-43f8-9a9c-60f95e9466ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT:  The quick brown fox jumps over the lazy dog .\n",
      "AUGMENTED TEXT:  ['The quick john brown fox jump over the lazy dog.']\n"
     ]
    }
   ],
   "source": [
    "text = 'The quick brown fox jumps over the lazy dog .'\n",
    "syn_aug = naw.SynonymAug(aug_src='wordnet',aug_max=2)\n",
    "syn_aug_text = syn_aug.augment(text,n=1)\n",
    "print(\"ORIGINAL TEXT: \", text)\n",
    "print(\"AUGMENTED TEXT: \",syn_aug_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50d3daeb-ac39-4fee-b838-c1f60102b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backTranslationAugmentation(data):\n",
    "    print('[backTranslationAugmentation] start')\n",
    "    duplicatedDf = pd.concat([data]*2, ignore_index=True)\n",
    "    \n",
    "    for i in range( int ( len(duplicatedDf)/2 ) , len(duplicatedDf)):\n",
    "        if(i==0):\n",
    "            print('Duplicate : ',duplicatedDf.loc[i,'sentence'])\n",
    "            print('Original : ',duplicatedDf.loc[i - int ( len(duplicatedDf)/2 ) ,'sentence'])\n",
    "        duplicatedDf.loc[i,'sentence'] = aug.augment( duplicatedDf.loc[i - int ( len(duplicatedDf)/2 ) ,'sentence'] )[0]\n",
    "        print('.',sep=\"\")\n",
    "    duplicatedDf.reset_index(drop=True,inplace=True)\n",
    "    print('[backTranslationAugmentation] end')\n",
    "    return duplicatedDf\n",
    "\n",
    "def synonymAugmentation(data):\n",
    "    print('[synonymAugmentation] start')\n",
    "    duplicatedDf = pd.concat([data]*2, ignore_index=True)\n",
    "    syn_aug = naw.SynonymAug(aug_src='wordnet',aug_max=2)\n",
    "    once = True\n",
    "    for i in range( int ( len(duplicatedDf)/2 ) , len(duplicatedDf)):\n",
    "        if(once):\n",
    "            print('Duplicate : ',duplicatedDf.loc[i,'sentence'])\n",
    "            print('Original : ',duplicatedDf.loc[i - int ( len(duplicatedDf)/2 ) ,'sentence'])\n",
    "            once = False\n",
    "        duplicatedDf.loc[i,'sentence'] = syn_aug.augment(duplicatedDf.loc[i - int ( len(duplicatedDf)/2 ) ,'sentence'],n=1)[0]\n",
    "        print('.',end=\"\")\n",
    "    duplicatedDf.reset_index(drop=True,inplace=True)\n",
    "    print('[synonymAugmentation] end')\n",
    "    return duplicatedDf\n",
    "\n",
    "def vectorizeAugmentAndSplit(allData):\n",
    "    \n",
    "    print(\"[vectorizeAndSplit] start\")\n",
    "    allData = preProcess(allData)\n",
    "    display(allData)\n",
    "    allData.reset_index(drop=True,inplace=True)\n",
    "    trainingData = allData[allData['set'] == 'train']\n",
    "    validationData = allData[allData['set'] == 'val']\n",
    "    testingData = allData[allData['set'] == 'test']\n",
    "\n",
    "    trainingData.reset_index(drop=True,inplace=True)\n",
    "    trainingData = synonymAugmentation(trainingData)\n",
    "    validationData.reset_index(drop=True,inplace=True)\n",
    "    testingData.reset_index(drop=True,inplace=True)\n",
    "    assert( len(trainingData)/2 + len(testingData) + len(validationData) == len(allData))\n",
    "    \n",
    "    Encoder = LabelEncoder()\n",
    "    trainY = Encoder.fit_transform(trainingData['annotation'])\n",
    "    testY = Encoder.fit_transform(testingData['annotation'])\n",
    "    \n",
    "    tfidVect = TfidfVectorizer(max_features=5000)\n",
    "    tfidVect.fit(trainingData['sentence'])\n",
    "    \n",
    "    trainXVectorized = tfidVect.transform(trainingData['sentence'])\n",
    "    testXVectorized = tfidVect.transform(testingData['sentence'])\n",
    "    \n",
    "    print(\"[vectorizeAndSplit] end\")\n",
    "    return trainXVectorized,testXVectorized,trainY,testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d295ec7-4a6b-49ea-97fc-2bd72c8414a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "allData = pd.DataFrame()\n",
    "dataDirectory = 'data'\n",
    "for filename in os.listdir(dataDirectory):\n",
    "    f = os.path.join(dataDirectory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        df = pd.read_csv(f,sep = '\\t',quoting=csv.QUOTE_NONE)\n",
    "        allData = pd.concat([allData,df])\n",
    "allData.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5282c78-53cb-4e16-8102-85d79f0f1b18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vectorizeAndSplit] start\n",
      "[preProcess] start\n",
      "But harming or killing another against his will , not by free contract , is clearly wrong ; if that is n't wrong , what is ?\n",
      "['but', 'harming', 'or', 'killing', 'another', 'against', 'his', 'will', ',', 'not', 'by', 'free', 'contract', ',', 'is', 'clearly', 'wrong', ';', 'if', 'that', 'is', \"n't\", 'wrong', ',', 'what', 'is', '?']\n",
      "[preProcess] end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic</th>\n",
       "      <th>retrievedUrl</th>\n",
       "      <th>archivedUrl</th>\n",
       "      <th>sentenceHash</th>\n",
       "      <th>sentence</th>\n",
       "      <th>annotation</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>abortion</td>\n",
       "      <td>http://2012election.procon.org/view.additional-resource.php?resourceID=004933</td>\n",
       "      <td>http://web.archive.org/web/20150415052859/http://2012election.procon.org/view.additional-resource.php?resourceID=004933</td>\n",
       "      <td>a1d2d5656a5029eb558812b8259b6567</td>\n",
       "      <td>['mean', 'steer', 'monetary', 'policy', 'keep', 'price', 'stable', 'b', 'keep', 'unemployment', 'low', 'economy', 'grow']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>abortion</td>\n",
       "      <td>http://www.listland.com/top-10-arguments-in-support-of-abortion/</td>\n",
       "      <td>http://web.archive.org/web/20160829133344/http://www.listland.com/top-10-arguments-in-support-of-abortion/</td>\n",
       "      <td>a4374eb8cae2c1d52499d0489c7bfb1d</td>\n",
       "      <td>['get']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>abortion</td>\n",
       "      <td>http://www.americamagazine.org/issue/feminist-case-against-abortion</td>\n",
       "      <td>http://web.archive.org/web/20160422223822/http://americamagazine.org:80/issue/feminist-case-against-abortion</td>\n",
       "      <td>825b1a5e0e7915950a2a4a657230d530</td>\n",
       "      <td>['nathanson', 'later', 'become']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>abortion</td>\n",
       "      <td>http://www.strangenotions.com/answering-three-common-arguments-for-abortion/</td>\n",
       "      <td>http://web.archive.org/web/20160916225634/http://www.strangenotions.com/answering-three-common-arguments-for-abortion/</td>\n",
       "      <td>644379f8e228f50f0871270164878c9b</td>\n",
       "      <td>['case', 'may', 'never', 'evil', 'directly', 'attack', 'kill', 'child', 'via', 'abortion', 'good', 'save', 'life', 'mother', 'may', 'result']</td>\n",
       "      <td>Argument_against</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>abortion</td>\n",
       "      <td>http://www.healthguidance.org/entry/13561/1/Pros-and-Cons-of-Abortion.html</td>\n",
       "      <td>http://web.archive.org/web/20160425042210/http://www.healthguidance.org:80/entry/13561/1/Pros-and-Cons-of-Abortion.html</td>\n",
       "      <td>51eefb36e8947e42403e336536cb00f0</td>\n",
       "      <td>['would', 'like', 'give', 'everyone', 'something', 'contemplate']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25487</th>\n",
       "      <td>3003</td>\n",
       "      <td>school uniforms</td>\n",
       "      <td>http://collegefootball.procon.org/</td>\n",
       "      <td>http://web.archive.org/web/20160621012142/http://collegefootball.procon.org:80/</td>\n",
       "      <td>34b6c3b9d4cb4bcad22bd525af887122</td>\n",
       "      <td>['playoff', 'would', 'remove', 'easy', 'schedule', 'make', 'championship', 'solely', 'performance']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25488</th>\n",
       "      <td>3004</td>\n",
       "      <td>school uniforms</td>\n",
       "      <td>http://debatepedia.idebate.org/en/index.php/Debate:_School_uniform</td>\n",
       "      <td>http://web.archive.org/web/20160709035437/http://debatepedia.idebate.org:80/en/index.php/Debate:_School_uniform</td>\n",
       "      <td>2489c5583b3489c2a931e40aba0176e7</td>\n",
       "      <td>['example', 'sikh', 'boy', 'orthodox', 'jew', 'islamic', 'girl', 'express', 'religious', 'belief', 'way', 'dress', 'uniform', 'stop']</td>\n",
       "      <td>Argument_against</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25489</th>\n",
       "      <td>3005</td>\n",
       "      <td>school uniforms</td>\n",
       "      <td>https://www.bostonglobe.com/metro/regionals/south/2014/02/20/uniforms-for-public-school-students-the-debate-continues-they-become-more-popular/btIYoFLoizOkFpXrdEUScK/story.html</td>\n",
       "      <td>http://web.archive.org/web/20160302063501/http://www.bostonglobe.com:80/metro/regionals/south/2014/02/20/uniforms-for-public-school-students-the-debate-continues-they-become-more-popular/btIYoFLoizOkFpXrdEUScK/story.html?</td>\n",
       "      <td>d32cd0154bd3d610fe5c8b255a30d39c</td>\n",
       "      <td>['lot', 'parental', 'input', 'say', 'bransfield']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25490</th>\n",
       "      <td>3006</td>\n",
       "      <td>school uniforms</td>\n",
       "      <td>http://www.educationbug.org/a/public-school-uniform-debate.html</td>\n",
       "      <td>http://web.archive.org/web/20160724094732/http://www.educationbug.org:80/a/public-school-uniform-debate.html</td>\n",
       "      <td>76498681c603d4c75690a2f25322106e</td>\n",
       "      <td>['school', 'uniform', 'ugly', 'unflattering', 'wear', 'something', 'unattractive', 'unflattering', 'good', 'student']</td>\n",
       "      <td>Argument_against</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25491</th>\n",
       "      <td>3007</td>\n",
       "      <td>school uniforms</td>\n",
       "      <td>https://journalistsresource.org/studies/society/education/school-uniforms-research-student-achievement-behavior</td>\n",
       "      <td>http://web.archive.org/web/20170126112226/https://journalistsresource.org/studies/society/education/school-uniforms-research-student-achievement-behavior</td>\n",
       "      <td>3a183cc644d9c5f790ea95c27c4f5e09</td>\n",
       "      <td>['public', 'school', 'nationwide', 'focus', 'improve', 'standardized', 'test', 'score', 'campus', 'safety', 'grow', 'number', 'begin', 'require', 'student', 'wear', 'uniforms', 'typically', 'polo', 'shirt', 'particular', 'color', 'pair', 'navy', 'khaki', 'pant', 'skirt', 'short']</td>\n",
       "      <td>NoArgument</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25492 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index            topic  \\\n",
       "0          0         abortion   \n",
       "1          1         abortion   \n",
       "2          2         abortion   \n",
       "3          3         abortion   \n",
       "4          4         abortion   \n",
       "...      ...              ...   \n",
       "25487   3003  school uniforms   \n",
       "25488   3004  school uniforms   \n",
       "25489   3005  school uniforms   \n",
       "25490   3006  school uniforms   \n",
       "25491   3007  school uniforms   \n",
       "\n",
       "                                                                                                                                                                           retrievedUrl  \\\n",
       "0                                                                                                         http://2012election.procon.org/view.additional-resource.php?resourceID=004933   \n",
       "1                                                                                                                      http://www.listland.com/top-10-arguments-in-support-of-abortion/   \n",
       "2                                                                                                                   http://www.americamagazine.org/issue/feminist-case-against-abortion   \n",
       "3                                                                                                          http://www.strangenotions.com/answering-three-common-arguments-for-abortion/   \n",
       "4                                                                                                            http://www.healthguidance.org/entry/13561/1/Pros-and-Cons-of-Abortion.html   \n",
       "...                                                                                                                                                                                 ...   \n",
       "25487                                                                                                                                                http://collegefootball.procon.org/   \n",
       "25488                                                                                                                http://debatepedia.idebate.org/en/index.php/Debate:_School_uniform   \n",
       "25489  https://www.bostonglobe.com/metro/regionals/south/2014/02/20/uniforms-for-public-school-students-the-debate-continues-they-become-more-popular/btIYoFLoizOkFpXrdEUScK/story.html   \n",
       "25490                                                                                                                   http://www.educationbug.org/a/public-school-uniform-debate.html   \n",
       "25491                                                                   https://journalistsresource.org/studies/society/education/school-uniforms-research-student-achievement-behavior   \n",
       "\n",
       "                                                                                                                                                                                                                         archivedUrl  \\\n",
       "0                                                                                                            http://web.archive.org/web/20150415052859/http://2012election.procon.org/view.additional-resource.php?resourceID=004933   \n",
       "1                                                                                                                         http://web.archive.org/web/20160829133344/http://www.listland.com/top-10-arguments-in-support-of-abortion/   \n",
       "2                                                                                                                       http://web.archive.org/web/20160422223822/http://americamagazine.org:80/issue/feminist-case-against-abortion   \n",
       "3                                                                                                             http://web.archive.org/web/20160916225634/http://www.strangenotions.com/answering-three-common-arguments-for-abortion/   \n",
       "4                                                                                                            http://web.archive.org/web/20160425042210/http://www.healthguidance.org:80/entry/13561/1/Pros-and-Cons-of-Abortion.html   \n",
       "...                                                                                                                                                                                                                              ...   \n",
       "25487                                                                                                                                                http://web.archive.org/web/20160621012142/http://collegefootball.procon.org:80/   \n",
       "25488                                                                                                                http://web.archive.org/web/20160709035437/http://debatepedia.idebate.org:80/en/index.php/Debate:_School_uniform   \n",
       "25489  http://web.archive.org/web/20160302063501/http://www.bostonglobe.com:80/metro/regionals/south/2014/02/20/uniforms-for-public-school-students-the-debate-continues-they-become-more-popular/btIYoFLoizOkFpXrdEUScK/story.html?   \n",
       "25490                                                                                                                   http://web.archive.org/web/20160724094732/http://www.educationbug.org:80/a/public-school-uniform-debate.html   \n",
       "25491                                                                      http://web.archive.org/web/20170126112226/https://journalistsresource.org/studies/society/education/school-uniforms-research-student-achievement-behavior   \n",
       "\n",
       "                           sentenceHash  \\\n",
       "0      a1d2d5656a5029eb558812b8259b6567   \n",
       "1      a4374eb8cae2c1d52499d0489c7bfb1d   \n",
       "2      825b1a5e0e7915950a2a4a657230d530   \n",
       "3      644379f8e228f50f0871270164878c9b   \n",
       "4      51eefb36e8947e42403e336536cb00f0   \n",
       "...                                 ...   \n",
       "25487  34b6c3b9d4cb4bcad22bd525af887122   \n",
       "25488  2489c5583b3489c2a931e40aba0176e7   \n",
       "25489  d32cd0154bd3d610fe5c8b255a30d39c   \n",
       "25490  76498681c603d4c75690a2f25322106e   \n",
       "25491  3a183cc644d9c5f790ea95c27c4f5e09   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                       sentence  \\\n",
       "0                                                                                                                                                                     ['mean', 'steer', 'monetary', 'policy', 'keep', 'price', 'stable', 'b', 'keep', 'unemployment', 'low', 'economy', 'grow']   \n",
       "1                                                                                                                                                                                                                                                                                       ['get']   \n",
       "2                                                                                                                                                                                                                                                              ['nathanson', 'later', 'become']   \n",
       "3                                                                                                                                                 ['case', 'may', 'never', 'evil', 'directly', 'attack', 'kill', 'child', 'via', 'abortion', 'good', 'save', 'life', 'mother', 'may', 'result']   \n",
       "4                                                                                                                                                                                                                             ['would', 'like', 'give', 'everyone', 'something', 'contemplate']   \n",
       "...                                                                                                                                                                                                                                                                                         ...   \n",
       "25487                                                                                                                                                                                       ['playoff', 'would', 'remove', 'easy', 'schedule', 'make', 'championship', 'solely', 'performance']   \n",
       "25488                                                                                                                                                     ['example', 'sikh', 'boy', 'orthodox', 'jew', 'islamic', 'girl', 'express', 'religious', 'belief', 'way', 'dress', 'uniform', 'stop']   \n",
       "25489                                                                                                                                                                                                                                         ['lot', 'parental', 'input', 'say', 'bransfield']   \n",
       "25490                                                                                                                                                                     ['school', 'uniform', 'ugly', 'unflattering', 'wear', 'something', 'unattractive', 'unflattering', 'good', 'student']   \n",
       "25491  ['public', 'school', 'nationwide', 'focus', 'improve', 'standardized', 'test', 'score', 'campus', 'safety', 'grow', 'number', 'begin', 'require', 'student', 'wear', 'uniforms', 'typically', 'polo', 'shirt', 'particular', 'color', 'pair', 'navy', 'khaki', 'pant', 'skirt', 'short']   \n",
       "\n",
       "             annotation    set  \n",
       "0            NoArgument    val  \n",
       "1            NoArgument  train  \n",
       "2            NoArgument    val  \n",
       "3      Argument_against  train  \n",
       "4            NoArgument   test  \n",
       "...                 ...    ...  \n",
       "25487        NoArgument  train  \n",
       "25488  Argument_against  train  \n",
       "25489        NoArgument  train  \n",
       "25490  Argument_against  train  \n",
       "25491        NoArgument  train  \n",
       "\n",
       "[25492 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[synonymAugmentation] start\n",
      "Duplicate :  ['get']\n",
      "Original :  ['get']\n",
      ".....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................[synonymAugmentation] end\n",
      "[vectorizeAndSplit] end\n"
     ]
    }
   ],
   "source": [
    "trainXVectorized,testXVectorized,trainY,testY = vectorizeAugmentAndSplit(allData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8deb66b-e820-49c3-80e4-d6562c9fd192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes F1 Score ->  0.6308475239772949\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(trainXVectorized,trainY)\n",
    "# predict the labels on validation dataset\n",
    "predictionsNB = Naive.predict(testXVectorized)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes F1 Score -> \",f1_score(predictionsNB, testY,pos_label='positive',average='micro'))\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(trainXVectorized,trainY)\n",
    "# predict the labels on validation dataset\n",
    "predictionsSVM = SVM.predict(testXVectorized)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM F1 Score -> \",f1_score(predictionsSVM, testY,average='micro')*100)\n",
    "model = XGBClassifier() \n",
    "model.fit(trainXVectorized, trainY)\n",
    "predXGBoost = model.predict(testXVectorized)\n",
    "print(\"XGBoost F1 Score -> \",f1_score(predXGBoost, testY,average='micro')*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f793736-dff7-431a-ae93-e970b2589908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
