{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a80ce98-ad0c-462e-a371-e64c81c9dfe9",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f818da48-3414-4baf-b719-20b541f43e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "### importing lazypredict library\n",
    "import lazypredict\n",
    "### importing LazyClassifier for classification problem\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "### importing LazyClassifier for classification problem because here we are solving Classification use case.\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "### importing breast Cancer Dataset from sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "### spliting dataset into training and testing part\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d37b48-24ea-4e64-a276-e9e19a14db67",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aa8b59c-eec4-40ac-a393-cbf1de90e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(data):\n",
    "    print(\"[preProcess] start\")\n",
    "    print(data['sentence'][2800])\n",
    "    data['sentence'] = [entry.lower() for entry in data['sentence']]\n",
    "    data['sentence'] = [word_tokenize(entry) for entry in data['sentence']]\n",
    "    print(data['sentence'][2800])\n",
    "    \n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    for index,entry in enumerate(data['sentence']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        finalWords = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        wordLemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = wordLemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                finalWords.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        data.loc[index,'sentence'] = str(finalWords)\n",
    "    print(\"[preProcess] end\")\n",
    "    return data\n",
    "\n",
    "def vectorizeAndSplit(allData):\n",
    "    \n",
    "    print(\"[vectorizeAndSplit] start\")\n",
    "    allData = preProcess(allData)\n",
    "    display(allData)\n",
    "    allData.reset_index(drop=True,inplace=True)\n",
    "    trainingData = allData[allData['set'] == 'train']\n",
    "    validationData = allData[allData['set'] == 'val']\n",
    "    testingData = allData[allData['set'] == 'test']\n",
    "\n",
    "    trainingData.reset_index(drop=True,inplace=True)\n",
    "    validationData.reset_index(drop=True,inplace=True)\n",
    "    testingData.reset_index(drop=True,inplace=True)\n",
    "    assert( len(trainingData) + len(testingData) + len(validationData) == len(allData))\n",
    "    \n",
    "    Encoder = LabelEncoder()\n",
    "    trainY = Encoder.fit_transform(trainingData['annotation'])\n",
    "    testY = Encoder.fit_transform(testingData['annotation'])\n",
    "    \n",
    "    tfidVect = TfidfVectorizer(max_features=5000)\n",
    "    tfidVect.fit(trainingData['sentence'])\n",
    "    \n",
    "    trainXVectorized = tfidVect.transform(trainingData['sentence'])\n",
    "    testXVectorized = tfidVect.transform(testingData['sentence'])\n",
    "    \n",
    "    print(\"[vectorizeAndSplit] end\")\n",
    "    return trainXVectorized,testXVectorized,trainY,testY\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9566d9-5b3d-4633-8a69-4fc10933918b",
   "metadata": {},
   "source": [
    "### Load all data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e9d86e-57f1-4b5d-9462-13efe6ccd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allData = pd.DataFrame()\n",
    "dataDirectory = 'data'\n",
    "for filename in os.listdir(dataDirectory):\n",
    "    f = os.path.join(dataDirectory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        df = pd.read_csv(f,sep = '\\t',quoting=csv.QUOTE_NONE)\n",
    "        allData = pd.concat([allData,df])\n",
    "allData.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799c8fd-149c-45e3-8bab-e5377b89d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXVectorized,testXVectorized,trainY,testY = vectorizeAndSplit(allData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b44e7b-ef3e-498c-b055-aa64cd262e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes F1 Score ->  0.6265413975337639\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(trainXVectorized,trainY)\n",
    "# predict the labels on validation dataset\n",
    "predictionsNB = Naive.predict(testXVectorized)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes F1 Score -> \",f1_score(predictionsNB, testY,pos_label='positive',average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6f3bff-221f-4172-ae62-e4347efff9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM F1 Score ->  62.86944607555295\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(trainXVectorized,trainY)\n",
    "# predict the labels on validation dataset\n",
    "predictionsSVM = SVM.predict(testXVectorized)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM F1 Score -> \",f1_score(predictionsSVM, testY,average='micro')*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6291cdf3-d0f6-4f41-b4cc-b7b563ed68e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost F1 Score ->  63.28048541789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = XGBClassifier() \n",
    "model.fit(trainXVectorized, trainY)\n",
    "predXGBoost = model.predict(testXVectorized)\n",
    "print(\"XGBoost F1 Score -> \",f1_score(predXGBoost, testY,average='micro')*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d537cbe2-4fbf-4f97-9341-269a99059a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, ..., 2, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainXVectorized.shape, trainY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba6847-5f39-4738-a1b4-8c7a82035b5e",
   "metadata": {},
   "source": [
    "# With Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585b8816-3689-48c6-86d0-aa8e139ae746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 67.0/67.0 [00:00<00:00, 67.0kB/s]\n",
      "Downloading (…)/main/vocab-src.json: 100%|██████████| 849k/849k [00:06<00:00, 132kB/s]\n",
      "Downloading (…)/main/vocab-tgt.json: 100%|██████████| 849k/849k [00:03<00:00, 243kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 315k/315k [00:01<00:00, 181kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT:  The quick brown fox jumps over the lazy dog .\n",
      "AUGMENTED TEXT:  ['The speedy brown fox jumps over the lazy dog.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aug = naw.BackTranslationAug()\n",
    "text = 'The quick brown fox jumps over the lazy dog .'\n",
    "\n",
    "print(\"ORIGINAL TEXT: \", text)\n",
    "print(\"AUGMENTED TEXT: \",aug.augment(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4663adfb-8e6c-43f8-9a9c-60f95e9466ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT:  The quick brown fox jumps over the lazy dog .\n",
      "AUGMENTED TEXT:  ['The quick john brown fox jump over the lazy dog.']\n"
     ]
    }
   ],
   "source": [
    "text = 'The quick brown fox jumps over the lazy dog .'\n",
    "syn_aug = naw.SynonymAug(aug_src='wordnet',aug_max=2)\n",
    "syn_aug_text = syn_aug.augment(text,n=1)\n",
    "print(\"ORIGINAL TEXT: \", text)\n",
    "print(\"AUGMENTED TEXT: \",syn_aug_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50d3daeb-ac39-4fee-b838-c1f60102b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backTranslationAugmentation(data):\n",
    "    print('[backTranslationAugmentation] start')\n",
    "    duplicatedDf = pd.concat([data]*2, ignore_index=True)\n",
    "    \n",
    "    for i in range( int ( len(duplicatedDf)/2 ) , len(duplicatedDf)):\n",
    "        if(i==0):\n",
    "            print('Duplicate : ',duplicatedDf.loc[i,'sentence'])\n",
    "            print('Original : ',duplicatedDf.loc[i - int ( len(duplicatedDf)/2 ) ,'sentence'])\n",
    "        duplicatedDf.loc[i,'sentence'] = aug.augment( duplicatedDf.loc[i - int ( len(duplicatedDf)/2 ) ,'sentence'] )[0]\n",
    "        print('.',sep=\"\")\n",
    "    duplicatedDf.reset_index(drop=True,inplace=True)\n",
    "    print('[backTranslationAugmentation] end')\n",
    "    return duplicatedDf\n",
    "\n",
    "def synonymAugmentation(data):\n",
    "    print('[synonymAugmentation] start')\n",
    "    duplicatedDf = pd.concat([data]*2, ignore_index=True)\n",
    "    syn_aug = naw.SynonymAug(aug_src='wordnet',aug_max=2)\n",
    "    once = True\n",
    "    for i in range( int ( len(duplicatedDf)/2 ) , len(duplicatedDf)):\n",
    "        if(once):\n",
    "            print('Duplicate : ',duplicatedDf.loc[i,'sentence'])\n",
    "            print('Original : ',duplicatedDf.loc[i - int ( len(duplicatedDf)/2 ) ,'sentence'])\n",
    "            once = False\n",
    "        duplicatedDf.loc[i,'sentence'] = syn_aug.augment(duplicatedDf.loc[i - int ( len(duplicatedDf)/2 ) ,'sentence'],n=1)[0]\n",
    "        print('.',end=\"\")\n",
    "    duplicatedDf.reset_index(drop=True,inplace=True)\n",
    "    print('[synonymAugmentation] end')\n",
    "    return duplicatedDf\n",
    "\n",
    "def vectorizeAugmentAndSplit(allData):\n",
    "    \n",
    "    print(\"[vectorizeAndSplit] start\")\n",
    "    allData = preProcess(allData)\n",
    "    display(allData)\n",
    "    allData.reset_index(drop=True,inplace=True)\n",
    "    trainingData = allData[allData['set'] == 'train']\n",
    "    validationData = allData[allData['set'] == 'val']\n",
    "    testingData = allData[allData['set'] == 'test']\n",
    "\n",
    "    trainingData.reset_index(drop=True,inplace=True)\n",
    "    trainingData = synonymAugmentation(trainingData)\n",
    "    validationData.reset_index(drop=True,inplace=True)\n",
    "    testingData.reset_index(drop=True,inplace=True)\n",
    "    assert( len(trainingData)/2 + len(testingData) + len(validationData) == len(allData))\n",
    "    \n",
    "    Encoder = LabelEncoder()\n",
    "    trainY = Encoder.fit_transform(trainingData['annotation'])\n",
    "    testY = Encoder.fit_transform(testingData['annotation'])\n",
    "    \n",
    "    tfidVect = TfidfVectorizer(max_features=5000)\n",
    "    tfidVect.fit(trainingData['sentence'])\n",
    "    \n",
    "    trainXVectorized = tfidVect.transform(trainingData['sentence'])\n",
    "    testXVectorized = tfidVect.transform(testingData['sentence'])\n",
    "    \n",
    "    print(\"[vectorizeAndSplit] end\")\n",
    "    return trainXVectorized,testXVectorized,trainY,testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d295ec7-4a6b-49ea-97fc-2bd72c8414a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "allData = pd.DataFrame()\n",
    "dataDirectory = 'data'\n",
    "for filename in os.listdir(dataDirectory):\n",
    "    f = os.path.join(dataDirectory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        df = pd.read_csv(f,sep = '\\t',quoting=csv.QUOTE_NONE)\n",
    "        allData = pd.concat([allData,df])\n",
    "allData.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5282c78-53cb-4e16-8102-85d79f0f1b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainXVectorized,testXVectorized,trainY,testY = vectorizeAugmentAndSplit(allData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8deb66b-e820-49c3-80e4-d6562c9fd192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes F1 Score ->  0.6308475239772949\n",
      "SVM F1 Score ->  61.79291446467019\n",
      "XGBoost F1 Score ->  63.848111176355445\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(trainXVectorized,trainY)\n",
    "# predict the labels on validation dataset\n",
    "predictionsNB = Naive.predict(testXVectorized)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes F1 Score -> \",f1_score(predictionsNB, testY,pos_label='positive',average='micro'))\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(trainXVectorized,trainY)\n",
    "# predict the labels on validation dataset\n",
    "predictionsSVM = SVM.predict(testXVectorized)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM F1 Score -> \",f1_score(predictionsSVM, testY,average='micro')*100)\n",
    "model = XGBClassifier() \n",
    "model.fit(trainXVectorized, trainY)\n",
    "predXGBoost = model.predict(testXVectorized)\n",
    "print(\"XGBoost F1 Score -> \",f1_score(predXGBoost, testY,average='micro')*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f793736-dff7-431a-ae93-e970b2589908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
